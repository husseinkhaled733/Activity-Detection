{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Data from Text Files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d7ce0160fbad2ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from Evaluation import compute_clusters, compute_purity, compute_recall, compute_f1, compute_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b515a3ce6b5657a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Data from Text Files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52467810d2bd3c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "training_data_means = []\n",
    "evaluation_data_means = []\n",
    "flattened_training_data = []\n",
    "flattened_evaluation_data = []\n",
    "training_labels = []\n",
    "evaluation_labels = []\n",
    "\n",
    "# Step 1: Accessing the Data Directory\n",
    "activities = os.listdir(data_directory)\n",
    "\n",
    "# Step 2: Iterating Through Activity Folders\n",
    "for activity in activities:\n",
    "    activity_path = os.path.join(data_directory, activity)\n",
    "    activity_number = int(activity.split(\"a\")[1])\n",
    "    \n",
    "    print(\"Activity Number:\", activity_number)\n",
    "    print(\"Activity:\", activity)\n",
    "    training_labels.extend([activity_number] * 48 * 8)  # 48 segments for training\n",
    "    evaluation_labels.extend([activity_number] * 12 * 8)  # 12 segments for evaluation\n",
    "    \n",
    "    \n",
    "    # Step 3: Iterating Through Subject Folders\n",
    "    subjects = os.listdir(activity_path)\n",
    "    for subject in subjects:\n",
    "        subject_path = os.path.join(activity_path, subject)\n",
    "        \n",
    "        # Step 4: Reading Text Files (Segments)\n",
    "        segments = os.listdir(subject_path)\n",
    "        training_segments = segments[:48]  # First 48 segments for training\n",
    "        evaluation_segments = segments[48:]  # Rest for evaluation\n",
    "        \n",
    "        for segment_file in training_segments:\n",
    "            segment_file_path = os.path.join(subject_path, segment_file)\n",
    "            with open(segment_file_path, 'r') as file:\n",
    "                segment_data = np.loadtxt(file, delimiter=',')\n",
    "                mean_data = np.mean(segment_data, axis=0)  # Taking mean along columns\n",
    "                training_data_means.append(mean_data)\n",
    "                \n",
    "                flattened_data = segment_data.flatten()  # Flattening the segment\n",
    "                flattened_training_data.append(flattened_data)\n",
    "        \n",
    "        for segment_file in evaluation_segments:\n",
    "            segment_file_path = os.path.join(subject_path, segment_file)\n",
    "            with open(segment_file_path, 'r') as file:\n",
    "                segment_data = np.loadtxt(file, delimiter=',')\n",
    "                mean_data = np.mean(segment_data, axis=0)  # Taking mean along columns\n",
    "                evaluation_data_means.append(mean_data)\n",
    "                \n",
    "                flattened_data = segment_data.flatten()  # Flattening the segment\n",
    "                flattened_evaluation_data.append(flattened_data)\n",
    "\n",
    "# Convert the data lists into numpy arrays\n",
    "training_data_means = np.array(training_data_means)\n",
    "evaluation_data_means = np.array(evaluation_data_means)\n",
    "flattened_training_data = np.array(flattened_training_data)\n",
    "flattened_evaluation_data = np.array(flattened_evaluation_data)\n",
    "PCA_reduction=PCA(n_components=0.9)\n",
    "PCA_training_data = PCA_reduction.fit_transform(flattened_training_data)\n",
    "PCA_evaluation_data = PCA_reduction.transform(flattened_evaluation_data)\n",
    "\n",
    "\n",
    "# Now, 'training_data_means' and 'evaluation_data_means' contain mean values of each column for each segment,\n",
    "# and 'flattened_training_data' and 'flattened_evaluation_data' contain flattened data for each segment.\n",
    "print(\"Shape of training labels\", np.array(training_labels).shape)\n",
    "print(\"Shape of evaluation labels\", np.array(evaluation_labels).shape)\n",
    "print(\"Shape of training data (means):\", training_data_means.shape)\n",
    "print(\"Shape of evaluation data (means):\", evaluation_data_means.shape)\n",
    "print(\"Shape of flattened training data:\", flattened_training_data.shape)\n",
    "print(\"Shape of flattened evaluation data:\", flattened_evaluation_data.shape)\n",
    "print(\"Shape of PCA training data:\", PCA_training_data.shape)\n",
    "print(\"Shape of PCA evaluation data:\", PCA_evaluation_data.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8c897df49c583aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Kmeans Clustering implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4525146c9de1253"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    def __init__(self, n_clusters, max_iter=300, tol=1e-4):\n",
    "        self.centroids = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.clusters = None\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.inertia_ = None\n",
    "        np.random.seed(42)\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Initialize centroids randomly\n",
    "        clusters = None\n",
    "        centroids_idx = np.random.choice(data.shape[0], self.n_clusters, replace=False)\n",
    "        self.centroids = data[centroids_idx]\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Assignment step\n",
    "            distances = np.linalg.norm(data[:, None] - self.centroids, axis=2)\n",
    "            clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "            # Update step\n",
    "            new_centroids = np.array([data[clusters == k].mean(axis=0) for k in range(self.n_clusters)])\n",
    "\n",
    "            # Check convergence\n",
    "            if np.linalg.norm(new_centroids - self.centroids) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        self.clusters = clusters\n",
    "        self.cluster_centers_ = self.centroids\n",
    "\n",
    "        # Calculate inertia\n",
    "        self.inertia_ = self._calculate_inertia(data)\n",
    "\n",
    "    def _calculate_inertia(self, data):\n",
    "        inertia = 0\n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = data[self.clusters == i]\n",
    "            distances = np.linalg.norm(cluster_points - self.centroids[i], axis=1)\n",
    "            inertia += np.sum(distances**2)\n",
    "        return inertia\n",
    "\n",
    "    def predict(self, data):\n",
    "        distances = np.linalg.norm(data[:, None] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29522bfebd0e54ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Applying Kmeans Clustering on the first approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "467c0283c0351b93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Ks = [8, 13, 19, 28, 38]\n",
    "means_score = []\n",
    "means_inertia = []\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(training_data_means)\n",
    "    means_score.append(metrics.silhouette_score(training_data_means, kmeans.clusters, metric='euclidean'))\n",
    "    means_inertia.append(kmeans.inertia_)\n",
    "    print(\"Silhouette Score:\", means_score[-1])\n",
    "    print(\"Inertia:\", kmeans.inertia_)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b0d72bebc751e61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Applying Kmeans Clustering on the second approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c95bf809f093878"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reduce_score = []\n",
    "reduce_inertia = []\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_training_data)\n",
    "    reduce_score.append(metrics.silhouette_score(PCA_training_data, kmeans.clusters, metric='euclidean'))\n",
    "    reduce_inertia.append(kmeans.inertia_)\n",
    "    print(\"Silhouette Score:\", reduce_score[-1])\n",
    "    print(\"Inertia:\", kmeans.inertia_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39dba6c6363ca318"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89908ac44834cae5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Silhouette Score vs Number of Clusters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6106db5e9ef2921"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Ks, means_score, marker='o', label='Mean Data')\n",
    "plt.plot(Ks, reduce_score, marker='x', label='PCA Data')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1000a38618d870b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot, we see that the silhouette score is higher for the mean data compared to the PCA data. This indicates that the mean data is more suitable for clustering compared to the PCA data. The silhouette score is highest for 38 clusters in mean data case while the highest for 8 clusters in the PCA data case."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c318ef847aa30904"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inertia vs Number of Clusters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec147cf1de629caa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Ks, means_inertia, marker='o', label='Mean Data')\n",
    "plt.plot(Ks, reduce_inertia, marker='x', label='PCA Data')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia vs Number of Clusters')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5252d6156ff87215"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot, we see that the inertia is higher for the PCA data compared to the mean data. This indicates that the mean data is more suitable for clustering compared to the PCA data. The inertia is highest for eight clusters in both cases."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36f4dfdd047c3dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cecb9f0812cb10fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observed that the mean data is more suitable for clustering compared to the PCA data because of the following:\n",
    "1- The silhouette score is higher for the mean data compared to the PCA data.\n",
    "2- The inertia is lower for the mean data compared to the PCA data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c30148183ea843"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a099440ee3d9eeb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Purity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62e532ec698873e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training data + Means approach\n",
    "print(\"Training data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(training_data_means)\n",
    "    print(f'# clusters = {K}:', compute_purity(compute_clusters(training_labels, kmeans.clusters, K), len(training_data_means)))\n",
    "\n",
    "# Training data + PCA approach\n",
    "print(\"Training data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_training_data)\n",
    "    print(f'# clusters = {K}:', compute_purity(compute_clusters(training_labels, kmeans.clusters, K), len(PCA_training_data)))\n",
    "\n",
    "# Evaluation data + Means approach\n",
    "print(\"Evaluation data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(evaluation_data_means)\n",
    "    print(f'# clusters = {K}:', compute_purity(compute_clusters(evaluation_labels, kmeans.clusters, K), len(evaluation_data_means)))\n",
    "\n",
    "# Evaluation data + PCA approach\n",
    "print(\"Evaluation data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_evaluation_data)\n",
    "    print(f'# clusters = {K}:', compute_purity(compute_clusters(evaluation_labels, kmeans.clusters, K), len(PCA_evaluation_data)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b2be6cdf1119b20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a1cc2fb35fee111"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training data + Means approach\n",
    "print(\"Training data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(training_data_means)\n",
    "    print(f'# clusters = {K}:', compute_recall(compute_clusters(training_labels, kmeans.clusters, K), len(training_data_means)))\n",
    "\n",
    "# Training data + PCA approach\n",
    "print(\"Training data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_training_data)\n",
    "    print(f'# clusters = {K}:', compute_recall(compute_clusters(training_labels, kmeans.clusters, K), len(PCA_training_data)))\n",
    "\n",
    "# Evaluation data + Means approach\n",
    "print(\"Evaluation data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(evaluation_data_means)\n",
    "    print(f'# clusters = {K}:', compute_recall(compute_clusters(evaluation_labels, kmeans.clusters, K), len(evaluation_data_means)))\n",
    "\n",
    "# Evaluation data + PCA approach\n",
    "print(\"Evaluation data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_evaluation_data)\n",
    "    print(f'# clusters = {K}:', compute_recall(compute_clusters(evaluation_labels, kmeans.clusters, K), len(PCA_evaluation_data)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e57e09ca1a31405"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## F1 Score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdf7fc37cf68b65e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training data + Means approach\n",
    "print(\"Training data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(training_data_means)\n",
    "    print(f'# clusters = {K}:', compute_f1(compute_clusters(training_labels, kmeans.clusters, K)))\n",
    "\n",
    "# Training data + PCA approach\n",
    "print(\"Training data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_training_data)\n",
    "    print(f'# clusters = {K}:', compute_f1(compute_clusters(training_labels, kmeans.clusters, K)))\n",
    "\n",
    "# Evaluation data + Means approach\n",
    "print(\"Evaluation data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(evaluation_data_means)\n",
    "    print(f'# clusters = {K}:', compute_f1(compute_clusters(evaluation_labels, kmeans.clusters, K)))\n",
    "\n",
    "# Evaluation data + PCA approach\n",
    "print(\"Evaluation data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_evaluation_data)\n",
    "    print(f'# clusters = {K}:', compute_f1(compute_clusters(evaluation_labels, kmeans.clusters, K)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07bc776cfa46d9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20f8dba1f47ce14d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training data + Means approach\n",
    "print(\"Training data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(training_data_means)\n",
    "    print(f'# clusters = {K}:', compute_entropy(compute_clusters(training_labels, kmeans.clusters, K), len(training_data_means)))\n",
    "\n",
    "# Training data + PCA approach\n",
    "print(\"Training data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_training_data)\n",
    "    print(f'# clusters = {K}:', compute_entropy(compute_clusters(training_labels, kmeans.clusters, K), len(PCA_training_data)))\n",
    "\n",
    "# Evaluation data + Means approach\n",
    "print(\"Evaluation data + Means approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(evaluation_data_means)\n",
    "    print(f'# clusters = {K}:', compute_entropy(compute_clusters(evaluation_labels, kmeans.clusters, K), len(evaluation_data_means)))\n",
    "\n",
    "# Evaluation data + PCA approach\n",
    "print(\"Evaluation data + PCA approach\")\n",
    "for K in Ks:\n",
    "    kmeans = Kmeans(n_clusters=K)\n",
    "    kmeans.fit(PCA_evaluation_data)\n",
    "    print(f'# clusters = {K}:', compute_entropy(compute_clusters(evaluation_labels, kmeans.clusters, K), len(PCA_evaluation_data)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "624f01fe81ef1d7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
