{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing\n",
    "This cell is only concerned with importing the libraries and methods needed for implementing spectral clustering."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading data\n",
    "To run the algorithm, we read the data in a multidimensional array of size 19 * 8 * 12 * 125 * 45."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "aps_eval = []\n",
    "temp = []\n",
    "for i in range(1, 20):\n",
    "    p_eval = []\n",
    "    for j in range(1, 9):\n",
    "        s_eval = []\n",
    "        for k in range (49, 61):\n",
    "            temp = []\n",
    "            path = \"data\\\\a\"\n",
    "            path += f'0{i}' if i < 10 else f'{i}'\n",
    "            path += f'\\\\p{j}\\\\s'\n",
    "            path += f'{k}.txt'\n",
    "            file = open(path, \"r\")\n",
    "            for l in range(125):\n",
    "                temp.append(np.array(file.readline().split(','), dtype=float))\n",
    "            s_eval.append(np.array(temp))\n",
    "        p_eval.append(np.array(np.array(s_eval)))\n",
    "    aps_eval.append(np.array(p_eval))\n",
    "aps_eval = np.array(aps_eval)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Points using means (first method)\n",
    "\n",
    "Here we loop on our array to create the points using the means of every column of a file as a feature, making a total of 1824 samples each with 45 features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points of the means method =  1824\n",
      "Number of features of the means method =  45\n"
     ]
    }
   ],
   "source": [
    "eval_points_means = []\n",
    "for a in range(19):\n",
    "    for p in range(8):\n",
    "        for s in range(12):\n",
    "            eval_points_means.append(np.mean(aps_eval[a][p][s], axis=0))\n",
    "eval_points_means = np.array(eval_points_means)\n",
    "print(\"Number of points of the means method = \", len(eval_points_means))\n",
    "print(\"Number of features of the means method = \", len(eval_points_means[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Points using flattening (second method)\n",
    "\n",
    "Here we loop on our array to create the points using each element of every file as a feature, making a total of 1824 samples each with 5625 features, so we perform PCA on them to reduce their dimensionality to about 135 feature each."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points of the flattening method =  1824\n",
      "Number of features of the flattening method =  136\n"
     ]
    }
   ],
   "source": [
    "eval_points_flattened = []\n",
    "for a in range(19):\n",
    "    for p in range(8):\n",
    "        for s in range(12):\n",
    "            eval_points_flattened.append([])\n",
    "            for r in range(125):\n",
    "                for n in range(45):\n",
    "                    eval_points_flattened[a*96+p*12+s].append(aps_eval[a][p][s][r][n])\n",
    "eval_points_flattened = PCA(n_components=0.85).fit_transform(eval_points_flattened)\n",
    "print(\"Number of points of the flattening method = \", len(eval_points_flattened))\n",
    "print(\"Number of features of the flattening method = \", len(eval_points_flattened[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the matrices B\n",
    "\n",
    "To calculate the matrix B for each dataset, we need first to calculate the similarity matrix for each of them, where we used th rbf kernel function, with passing the gamma parameter with value 0.00001, then we minus the degree matrix with it getting the laplacian matrix, and multiplying it with the inverse degree matrix."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "# Similarity matrix for the first method\n",
    "sim_mat_means = rbf_kernel(eval_points_means, eval_points_means, 0.00001)\n",
    "for i in range(len(sim_mat_means)):\n",
    "    # Getting degree of a data point\n",
    "    x = np.sum(sim_mat_means[i])\n",
    "    # Subtracting from the diagonal matrix\n",
    "    sim_mat_means[i][i] += -x\n",
    "    sim_mat_means[i] *= -1\n",
    "    # Multiplying by the inverse diagonal matrix\n",
    "    sim_mat_means[i] /= x\n",
    "\n",
    "# Similarity matrix for the second method\n",
    "sim_mat_flattened = rbf_kernel(eval_points_flattened, eval_points_flattened, 0.00001)\n",
    "for i in range(len(sim_mat_flattened)):\n",
    "    # Getting degree of a data point\n",
    "    x = np.sum(sim_mat_flattened[i])\n",
    "    # Subtracting from the diagonal matrix\n",
    "    sim_mat_flattened[i][i] += -x\n",
    "    sim_mat_flattened[i] *= -1\n",
    "    # Multiplying by the inverse diagonal matrix\n",
    "    sim_mat_flattened[i] /= x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting points for clustering\n",
    "\n",
    "To get the points to be clustered, we need the eigen vectors and eigen values of the B matrix, then we get the least k eigen values, and the eigen vectors corresponding to them, they will make the columns for our data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "# Getting eigen values and eigen vectors of the means method\n",
    "eigen_values, eigen_vectors = np.linalg.eig(sim_mat_means)\n",
    "# Getting the sorted indices of the eigen values\n",
    "idx = np.real(eigen_values).argsort()[::-1]\n",
    "# Sorting the eigen vectors according to their ascending eigen values\n",
    "eigen_vectors = np.real(np.flip(np.array(eigen_vectors.transpose()[idx, :])))\n",
    "# Getting the first 19 eigen vectors as columns\n",
    "data_means = np.flip(np.array(eigen_vectors[:19]).transpose())\n",
    "\n",
    "# Getting eigen values and eigen vectors of the means method\n",
    "eigen_values, eigen_vectors = np.linalg.eig(sim_mat_flattened)\n",
    "# Getting the sorted indices of the eigen values\n",
    "idx = np.real(eigen_values).argsort()[::-1]\n",
    "# Sorting the eigen vectors according to their ascending eigen values\n",
    "eigen_vectors = np.real(np.flip(np.array(eigen_vectors.transpose()[idx, :])))\n",
    "# Getting the first 19 eigen vectors as columns\n",
    "data_flattened = np.flip(np.array(eigen_vectors[:19]).transpose())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalizing the rows\n",
    "\n",
    "Each row represents a point in our space, and they need to be normalized before performing the kmeans on them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "for i in range(len(data_means)):\n",
    "    # Getting the norm of the row\n",
    "    vec_sum = np.linalg.norm(data_means[i])\n",
    "    # Normalizing the row\n",
    "    data_means[i] /= vec_sum\n",
    "\n",
    "for i in range(len(data_flattened)):\n",
    "    # Getting the norm of the row\n",
    "    vec_sum = np.linalg.norm(data_flattened[i])\n",
    "    # Normalizing the row\n",
    "    data_flattened[i] /= vec_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing kmeans\n",
    "\n",
    "The built-in kmeans method is used to cluster those points, and the labels are written in files to be used for evaluation if needed, and we calculate the silhouette score as a representation of the difference in accuracies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1724464491082588\n",
      "0.2646674903835613\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=19)\n",
    "kmeans.fit(data_means)\n",
    "print(metrics.silhouette_score(data_means, kmeans.labels_))\n",
    "labels_file = open(\"labels-means.txt\", \"w\")\n",
    "for i in range(19):\n",
    "    for j in range(8):\n",
    "        for q in range(12):\n",
    "            labels_file.write(str(kmeans.labels_[i*96+j*12+q]))\n",
    "            labels_file.write(\" \")\n",
    "        labels_file.write('\\n')\n",
    "    labels_file.write('\\n')\n",
    "labels_file.close()\n",
    "kmeans = KMeans(n_clusters=19)\n",
    "kmeans.fit(data_flattened)\n",
    "print(metrics.silhouette_score(data_flattened, kmeans.labels_))\n",
    "labels_file = open(\"labels-flattened.txt\", \"w\")\n",
    "for i in range(19):\n",
    "    for j in range(8):\n",
    "        for q in range(12):\n",
    "            labels_file.write(str(kmeans.labels_[i*96+j*12+q]))\n",
    "            labels_file.write(\" \")\n",
    "        labels_file.write('\\n')\n",
    "    labels_file.write('\\n')\n",
    "labels_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}