{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b42f6ba-806d-4e76-a75a-7370ebbb0372",
   "metadata": {},
   "source": [
    "Density-Based Spatial Clustering of Applications with Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e059e0-7f36-4cfd-931a-abfcb424d822",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9020bad7-7a2c-4cce-b25f-a8b84e206f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from Evaluation import compute_clusters, compute_purity, compute_recall, compute_f1, compute_entropy\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcb9a5",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684e0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData_means_flattened():\n",
    "    training_data_means = []\n",
    "    evaluation_data_means = []\n",
    "    flattened_training_data = []\n",
    "    flattened_evaluation_data = []\n",
    "    training_labels = []\n",
    "    evaluation_labels = []\n",
    "    data_directory = \"C:\\\\Users\\\\DELL\\\\Downloads\\\\daily+and+sports+activities\\\\data\"\n",
    "\n",
    "    # Step 1: Accessing the Data Directory\n",
    "    activities = os.listdir(data_directory)\n",
    "\n",
    "    # Step 2: Iterating Through Activity Folders\n",
    "    for activity in activities:\n",
    "        activity_path = os.path.join(data_directory, activity)\n",
    "        activity_number = int(activity.split(\"a\")[1])\n",
    "\n",
    "        # Assigning labels\n",
    "        training_labels.extend([activity_number] * 48 * 8)\n",
    "        evaluation_labels.extend([activity_number] * 12 * 8)\n",
    "\n",
    "        # Step 3: Iterating Through Subject Folders\n",
    "        subjects = os.listdir(activity_path)\n",
    "        for subject in subjects:\n",
    "            subject_path = os.path.join(activity_path, subject)\n",
    "\n",
    "            # Step 4: Reading Text Files (Segments)\n",
    "            segments = os.listdir(subject_path)\n",
    "            training_segments = segments[:48]  # First 48 segments for training\n",
    "            evaluation_segments = segments[48:]  # Rest for evaluation\n",
    "\n",
    "            for segment_file in training_segments:\n",
    "                segment_file_path = os.path.join(subject_path, segment_file)\n",
    "                with open(segment_file_path, 'r') as file:\n",
    "                    segment_data = np.loadtxt(file, delimiter=',')\n",
    "                    mean_data = np.mean(segment_data, axis=0)  # Taking mean along columns\n",
    "                    training_data_means.append(mean_data)\n",
    "\n",
    "                    flattened_data = segment_data.flatten()  # Flattening the segment\n",
    "                    flattened_training_data.append(flattened_data)\n",
    "\n",
    "            for segment_file in evaluation_segments:\n",
    "                segment_file_path = os.path.join(subject_path, segment_file)\n",
    "                with open(segment_file_path, 'r') as file:\n",
    "                    segment_data = np.loadtxt(file, delimiter=',')\n",
    "                    mean_data = np.mean(segment_data, axis=0)  # Taking mean along columns\n",
    "                    evaluation_data_means.append(mean_data)\n",
    "\n",
    "                    flattened_data = segment_data.flatten()  # Flattening the segment\n",
    "                    flattened_evaluation_data.append(flattened_data)\n",
    "\n",
    "    # Convert the data lists into numpy arrays\n",
    "    training_data_means = np.array(training_data_means)\n",
    "    evaluation_data_means = np.array(evaluation_data_means)\n",
    "    flattened_training_data = np.array(flattened_training_data)\n",
    "    flattened_evaluation_data = np.array(flattened_evaluation_data)\n",
    "\n",
    "    # Apply PCA reduction\n",
    "    PCA_reduction = PCA(n_components=0.9)\n",
    "    PCA_training_data = PCA_reduction.fit_transform(flattened_training_data)\n",
    "    PCA_evaluation_data = PCA_reduction.transform(flattened_evaluation_data)\n",
    "\n",
    "    # Print dimensions\n",
    "    print(\"Training Data Means Shape:\", training_data_means.shape)\n",
    "    print(\"Evaluation Data Means Shape:\", evaluation_data_means.shape)\n",
    "    print(\"PCA Training Data Shape:\", PCA_training_data.shape)\n",
    "    print(\"PCA Evaluation Data Shape:\", PCA_evaluation_data.shape)\n",
    "    print(\"Training Labels Shape:\", len(training_labels))\n",
    "    print(\"Evaluation Labels Shape:\", len(evaluation_labels))\n",
    "\n",
    "    return training_data_means, evaluation_data_means, PCA_training_data, PCA_evaluation_data, training_labels, evaluation_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9b4fd",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaab7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(dataset, eps, min_pts):\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "\n",
    "    for point_index, point in enumerate(dataset):\n",
    "        if point_index in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(point_index)\n",
    "        neighbors = region_query(dataset, point_index, eps)\n",
    "\n",
    "        if len(neighbors) >= min_pts:\n",
    "            cluster = []\n",
    "            expand_cluster(dataset, visited, neighbors, cluster, eps, min_pts)\n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters, extract_labels(clusters, len(dataset))\n",
    "\n",
    "def expand_cluster(dataset, visited, neighbors, cluster, eps, min_pts):\n",
    "    for index in neighbors:\n",
    "        if index not in visited:\n",
    "            cluster.append(index)\n",
    "    queue = deque(neighbors)\n",
    "\n",
    "    while queue:\n",
    "        current_point_index = queue.popleft()\n",
    "        current_point_neighbors = []\n",
    "        if current_point_index not in visited:\n",
    "            visited.add(current_point_index)\n",
    "            current_point_neighbors = region_query(dataset, current_point_index, eps)\n",
    "            if len(current_point_neighbors) >= min_pts:\n",
    "                queue.extend(current_point_neighbors)\n",
    "        for neighbor in current_point_neighbors:\n",
    "            if neighbor not in cluster:\n",
    "                cluster.append(neighbor)\n",
    "\n",
    "def region_query(dataset, query_point_index, eps):\n",
    "    neighbors = []\n",
    "    for index, point in enumerate(dataset):\n",
    "        if np.linalg.norm(point - dataset[query_point_index]) <= eps:\n",
    "            neighbors.append(index)\n",
    "    return neighbors\n",
    "\n",
    "def extract_labels(clusters, n):\n",
    "    labels = np.zeros(n, dtype=int) - 1\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for point_index in cluster:\n",
    "            labels[point_index] = i\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3ee89",
   "metadata": {},
   "source": [
    "Run DBSCAN and evaluate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f57f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Means Shape: (7296, 45)\n",
      "Evaluation Data Means Shape: (1824, 45)\n",
      "PCA Training Data Shape: (7296, 342)\n",
      "PCA Evaluation Data Shape: (1824, 342)\n",
      "Training Labels Shape: 7296\n",
      "Evaluation Labels Shape: 1824\n",
      "Method 1 Clusters at eps =  2.4 :  13\n",
      "Method 1 Labels at eps =  2.4 :  [-1 -1 -1 ... -1 -1 -1]\n",
      "Method 1 Purity =  0.17763157894736842\n",
      "Method 1 Recall =  0.7583150584795322\n",
      "Method 1 F1 Score =  0.2387641922954984\n",
      "Method 1 Conditional Entropy =  3.3935857413535184\n",
      "Method 2 Clusters at eps =  2.4 :  0\n",
      "Method 2 Labels at eps =  2.4 :  [-1 -1 -1 ... -1 -1 -1]\n",
      "Method 2 Purity =  0.05263157894736842\n",
      "Method 2 Recall =  1.0\n",
      "Method 2 F1 Score =  0.1\n",
      "Method 2 Conditional Entropy =  4.247927513443583\n"
     ]
    }
   ],
   "source": [
    "(training_data_means, evaluation_data_means, PCA_training_data, PCA_evaluation_data, training_labels,\n",
    " evaluation_labels) = readData_means_flattened()\n",
    "\n",
    "eps = 2.4\n",
    "\n",
    "# First Approach using means\n",
    "clusters, labels = dbscan(evaluation_data_means, eps, 13)\n",
    "print(\"Method 1 Clusters at eps = \", eps, \": \", len(clusters))\n",
    "print(\"Method 1 Labels at eps = \", eps, \": \", labels)\n",
    "print(\"Method 1 Purity = \",\n",
    "      compute_purity(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(evaluation_data_means)))\n",
    "print(\"Method 1 Recall = \",\n",
    "      compute_recall(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(evaluation_data_means)))\n",
    "print(\"Method 1 F1 Score = \", compute_f1(compute_clusters(evaluation_labels, labels, len(np.unique(labels)))))\n",
    "print(\"Method 1 Conditional Entropy = \",\n",
    "      compute_entropy(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(evaluation_data_means)))\n",
    "\n",
    "# Second Approach using PCA\n",
    "clusters, labels = dbscan(PCA_evaluation_data, eps, 13)\n",
    "print(\"Method 2 Clusters at eps = \", eps, \": \", len(clusters))\n",
    "print(\"Method 2 Labels at eps = \", eps, \": \", labels)\n",
    "\n",
    "print(\"Method 2 Purity = \",\n",
    "      compute_purity(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(PCA_evaluation_data)))\n",
    "print(\"Method 2 Recall = \",\n",
    "      compute_recall(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(PCA_evaluation_data)))\n",
    "print(\"Method 2 F1 Score = \", compute_f1(compute_clusters(evaluation_labels, labels, len(np.unique(labels)))))\n",
    "print(\"Method 2 Conditional Entropy = \",\n",
    "      compute_entropy(compute_clusters(evaluation_labels, labels, len(np.unique(labels))), len(PCA_evaluation_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
